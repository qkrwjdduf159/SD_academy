{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "텍스트 분석.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3hmyq0QphHq"
      },
      "source": [
        "#  NLP(Natural Language Processing) 와 텍스트 분석(Text analysis,Text mining))\n",
        "\n",
        "\n",
        "참고 : 파이썬 머신러닝 완벽 가이드\n",
        "\n",
        "## NLP\n",
        "- 인간의 언어를 이해하고 해석하는데 저 중점을 두고 기술이 발전 해 옴.\n",
        "- NLP 기술의 발전으로 텍스트 분석도 더욱 정교하게 발전\n",
        "\n",
        "## Text analysis\n",
        "- 머신러닝, 언어 이해, 통계 등을 활용해 모델을 수립하고 정보를 추출해 Business Intelligence)나 예측 분석 등의 분석 작업을 주로 수행.\n",
        "\n",
        "\n",
        "참고 : https://kerneler.tistory.com/142\n",
        "\n",
        "\n",
        "- 텍스트 마이닝은 비,반 정형 텍스트 데이터를 자연어 처리 기술에 기반하여 유용한 정보를 추출하여 가공하는 것을 목적으로 하는 기술.\n",
        "\n",
        "- 기술을 통하여 방대한 텍스트 뭉치에서 의미있는 정보를 추출해 내고, 다른 정보와의 연계성을 파악하며, 텍스트가 가진 카테고리를 찾아내는 등 단순한 정보 검색 그 이상의 결과를 얻어 낼 수 있다. 텍스트 마이닝 과정은 다음과 같은 단계를 거친다\n",
        "\n",
        "\n",
        "\n",
        "### **텍스트 문서-> 전처리 ->의미정보 변환 -> 의미정보 추출 -> 패턴 및 경향 분석 -> 정보 교환 및 평가**\n",
        "\n",
        "\n",
        "\n",
        "- 텍스트 분석에는 구조화 과정이 필요한데, 구조화된 데이터(ex. 수치)는 컴퓨터가 직접적으로 처리 할 수 있지만, 텍스트 데이터는 구조화된 수치 데이터와 달리 자연어로 구성되어 있기 때문에 비 구조적 데이터. 그리하여 텍스트 데이터(문서-문단-문장-단어)를 바로 처리 할 수 없다. 텍스트를 분석하려면, 데이터는 컴퓨터가 처리 할 수 있는 구조로 표현해 주어야 함.\n",
        "\n",
        "- 텍스트 데이터는 '구조화 시키는 과정'이 필수적.\n",
        "\n",
        "\n",
        "### 텍스트 마이닝 응용 분야\n",
        "\n",
        "1. 분류(Categorization)\n",
        "- 주어진 텍스트의 내용을 분석 해, 사전에 정의된 범주를 적절히 부여하는 과정\n",
        "\n",
        "ex)뉴스 기사 분석->정치/사회/생활/스포츠/IT/연예 등의 범주로 자동 분류\n",
        "\n",
        "- 다량의 문서는 내용들을 직접 일일이 파악하고, 분류하기에는 많은 시간이 소요, 또한 전문적인 내용의 경우 세부 분류는 전문가만이 수행이 가능함. ->이럴때 텍스트 마이닝을 쓰는게 적절함.\n",
        "\n",
        "- 분류 기법 : 문서 유사도, 패턴 인식, 정보 검색 기반\n",
        "\n",
        "2. 감성 분석(Sentiment Analysis) : 텍스트에서 나타나는 감정/판단/믿음/의견/기분 등의 주관적인 요소를 분석하는 기법. 소셜 미디어 감정 분석, 영화나 제품에 대한 긍정 또는 리뷰, 여론조사 의견 분석 등의 다양한 영역에서 활용 됨.\n",
        "\n",
        "2. 요약(Summarization)\n",
        "- 문서의 전체 내용을 대표할 수 있는 내용을 추출하는 과정 / 다량/대용량 문서의 내용 파악을 도와 필요한 문서에 접근하는 시간을 단축 해 줄 수 있음.\n",
        "\n",
        "-요약 기법 : 토픽 모델링, 표면 수준접근(surface level approach), 개체 수준 접근(entiry level approach), 화법수준접근(discourse level approach)\n",
        "\n",
        "3. 군집화(Clustering)\n",
        "- 텍스트 집단을 내용의 유사도에 따라 여러 개의 소집단으로 분할하는 과정\n",
        "- 군집화를 통해 텍스트 분류의 예비 작업(범주체계 정의, 표본 텍스트 관리)을 자동화시켜줄 수 있다.\n",
        "\n",
        "\n",
        "\n",
        "#### Text문서를 Feature Vectorization을 해야 되는데 대표적인 두 방법이다.\n",
        "\n",
        "<figure>\n",
        "<img src = 'https://statkclee.github.io/ds-authoring/fig/nlp-python-korean.png'>\n",
        "<figure>\n",
        "\n",
        "출처 : https://statkclee.github.io/ds-authoring/ds_kpf.html\n",
        "\n",
        "### Word2Vec\n",
        "\n",
        "<figure>\n",
        "<img src ='https://miro.medium.com/max/3902/1*hELlVp9hmZbDZVFstS61pg.png'>\n",
        "<figure>\n",
        "\n",
        "출처 : https://medium.com/analytics-vidhya/implementing-word2vec-in-tensorflow-44f93cf2665f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UvBMD8nvzbs"
      },
      "source": [
        "**말뭉치 또는 코퍼스(Corpus)는 자연언어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집합이다.**\n",
        "\n",
        "# 파이썬 기반의 NLP, 텍스트 분석만을 위한 패키지\n",
        "\n",
        "- NLTK(National Language Toolkit for python) : 파이썬의 가장 대표적인 NLP 패키지. 방대한 데이터 세트와 서브 모듈을 가지고 있으며 NLP의 거의 모든 영역을 커버하고 있음. 많은 NLP 패키지가 NLTK의 영향을 받아 작성되고 있음. 수행속도 측면에서 아쉬운 부분이 있어서 실제 대량의 데이터 기반에서는 제대로 활용되지 못하고 있음.\n",
        "\n",
        "- Gensim : 토픽 모델링 분야 에서 가장 두각을 나타내는 패키지. 오래전부터 토픽 모델링을 쉽게 구현할 수 있는 기능을 제공해왔으며, Word2Vec 구현 등의 다양한 신기능도 제공. SpaCy와 함께 가장 많이 사용되는 NLP 패키지.\n",
        "\n",
        "- SpaCy : 뛰어난 수행 성능으로 최근 가장 주목을 받는 NLP 패키지. 많은 NLP 애플리케이션에서 SpaCy를 사용하는 사례가 늘고 있음.\n",
        "\n",
        "- Word Colud : '핵심 단어'시각화 하는 기법. 문서의 키워드, 개념 등을 직관적으로 파악할 수 있도록 핵심 단어를 시각적으로 돋보이게 하는 기법.\n",
        "\n",
        "**토큰(token)이란 문법적으로 더 이상 나눌 수 없는 언어요소를 뜻함. 텍스트 토큰화(Text Toeknization)란 말뭉치로부터 토큰을 분리하는 작업을 뜻함.**\n",
        "\n",
        "## 텍스트 전처리 - 텍스트 정규화\n",
        "\n",
        "- Cleansing : 텍스트에서 분석에 오히려 방해가 되는 불필요한 문자, 기호 등을 사전에 제거하는 작업. 예를 들어 HTML, XML 태크나 특정 기호 등을 사전에 제거\n",
        "\n",
        "- Tokenization : 문장 토큰화, 단어 코튼화, N-gram\n",
        "\n",
        "- 필터링 /스톱 워드 제거 / 철자 수정 : 불필요한 단어나 분석에 큰 의미가 없는 단어(a, the, is, will등) 그리고 잘못된 철자 수정\n",
        "\n",
        "- Stemming / Lemmatization : 어근(단어 원형) 추출, Lemmatization이 Stemming보다 정교하고 의미론적 기반에서 단어원형을 찾아줌.\n",
        "\n",
        "### N-gram\n",
        "\n",
        "- 문장을 개별 단어 별로 하나씩 토큰화 할 경우 문맥적인 의미는 무시 될 수 밖에 없음. 이러한 문제를 조금이라도 해결해보자->N-garm 도입\n",
        "\n",
        "- N-gram은 연속된 n개의 단어를 하나의 토큰화 단위로 분리하는 것. n개 단어 크기 윈도우를 만들어 문장의 처음부터 오른쪽으로 움직이면서 토큰화 수행.\n",
        "\n",
        "ex) 'Here is a dog'라는 문장을 문자 절단 단위의 \"Her\", \"ere\", \"re_\", \"e_i\", \"_is\", \"is_\", \"s_a\", \"_a_\", \"a_d\", \"_do\", \"dog\" 로 만들 수 있다. \n",
        "같은 작업을 단어 단위의 2-gram으로 만든다면,\n",
        "\"Here is\", \"is a\", \"a dog\" 가 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jjfLeC6AgJg"
      },
      "source": [
        "## 토큰화에서 고려해야할 사항.\n",
        "\n",
        "토큰화 작업은 단순하게 코퍼스에서 구두점을 제외하고 공백 기준으로 잘라내는 작업이라고 간주할 수 없음. 섬세하게 알고리즘이 필요함.\n",
        "\n",
        "1. 구두점이나 특수문자를 단순 제외해서는 안됨.\n",
        "- 코퍼스에서 단어들을 걸러낼떄, 구두점이나 특수 문자를 단순히 제외하는 것은 옳지 않음.\n",
        "- 코퍼스에 대한 정제 작업을 진행하다보면, 구두점조차도 하나의 토큰으로 분류하기도 함.\n",
        "\n",
        "2. 줄임말과 단어 내에 띄어쓰기가 있는 경우\n",
        "- 토큰화 작업에서 종종 영어권 언어의 아포스트로피(')는 압축된 단어를 다시 펼치는 역할\n",
        "\n",
        "3. 표준 토큰화 예제\n",
        "- 표준으로 쓰이고 있는 토큰화 방법 중 하나인 Penn Treebank Tokenization의 규칙에 대해서 소개하고, 토큰화의 결과를 보도록 하겠습니다.\n",
        "\n",
        "- 규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.\n",
        "- 규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlA91G8YMeps",
        "outputId": "a1aad9e8-b488-4aa1-c74c-f71c4e27031b"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer=TreebankWordTokenizer()\n",
        "text=\"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
        "print(tokenizer.tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlDWit2y1wvK"
      },
      "source": [
        "## 텍스트 전처리 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v48yxOepfW-",
        "outputId": "dc7aacb9-3daf-4dad-9d38-b8421c18f823"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "128VQ6ad3hDJ"
      },
      "source": [
        "- Text Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCdUr7QH3PhF",
        "outputId": "b8e77b5b-02c9-4e16-9a22-1310b59d021a"
      },
      "source": [
        "from nltk import sent_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
        "               You can see it out your window or on your television. \\\n",
        "               You feel it when you go to work, or go to church or pay your taxes.'\n",
        "sentences = sent_tokenize(text=text_sample)\n",
        "print(type(sentences),len(sentences))\n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "<class 'list'> 3\n",
            "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkHKX5VT3lGa",
        "outputId": "d03a7eb4-acd5-45e1-a6d9-c9e3c907a1a6"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
        "words = word_tokenize(sentence)\n",
        "print(type(words), len(words))\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> 15\n",
            "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_BeaZaQ3n0w",
        "outputId": "83e44790-98b2-4d8e-a0f1-5e83534d0f05"
      },
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "#여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화 만드는 함수 생성\n",
        "def tokenize_text(text):\n",
        "    \n",
        "    # 문장별로 분리 토큰\n",
        "    sentences = sent_tokenize(text)\n",
        "    # 분리된 문장별 단어 토큰화\n",
        "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "    return word_tokens\n",
        "\n",
        "#여러 문장들에 대해 문장별 단어 토큰화 수행. \n",
        "word_tokens = tokenize_text(text_sample)\n",
        "print(type(word_tokens),len(word_tokens))\n",
        "print(word_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> 3\n",
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbx4m61iMxIU"
      },
      "source": [
        "참고 : 딥러닝을 위한 자연어 처리 입문(https://wikidocs.net/21698)\n",
        "\n",
        "# 한국어에서의 토큰화의 어려움\n",
        "\n",
        "1) 한국어는 교착어.\n",
        "- 대부분의 한국어 NLP에서 조사는 분리해야 함.\n",
        "- 띄어쓰기 단위가 영어처럼 독립적인 단어라면 띄어쓰기 단위로 토큰화를 하면 되겠지만 한국어는 어절이 독립적인 단어로 구성되는 것이 아니라 조사 등의 무언가가 붙어있는 경우가 많아서 이를 전부 분리해줘야 한다는 의미.\n",
        "\n",
        "- 한국어 토큰화에서는 형태소(morpheme)란 개념을 이해해야 함. 형태소(morpheme)란 뜻을 가진 가장 작은 말의 단위를 말함.이 형태소에는 두 가지 형태소가 있는데 자립 형태소와 의존 형태소입니다.\n",
        "\n",
        "- 자립 형태소 : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소. 그 자체로 단어가 된다. 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등이 있다.\n",
        "- 의존 형태소 : 다른 형태소와 결합하여 사용되는 형태소. 접사, 어미, 조사, 어간를 말한다.\n",
        "\n",
        "예를 들어 다음과 같은 문장이 있다고 합시다.\n",
        "\n",
        "- 문장 : 에디가 딥러닝책을 읽었다\n",
        "이를 형태소 단위로 분해하면 다음과 같습니다.\n",
        "\n",
        "- 자립 형태소 : 에디, 딥러닝책\n",
        "- 의존 형태소 : -가, -을, 읽-, -었, -다\n",
        "\n",
        "이를 통해 유추할 수 있는 것은 한국어에서 영어에서의 단어 토큰화와 유사한 형태를 얻으려면 어절 토큰화가 아니라 형태소 토큰화를 수행해야한다는 겁니다.\n",
        "\n",
        "2) 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다.\n",
        "\n",
        "- 사용하는 한국어 코퍼스가 뉴스 기사와 같이 띄어쓰기를 철저하게 지키려고 노력하는 글이라면 좋겠지만, 많은 경우에 띄어쓰기가 틀렸거나, 지켜지지 않는 코퍼스가 많습니다.\n",
        "\n",
        "- 한국어는 영어권 언어와 비교하여 띄어쓰기가 어렵고, 또 잘 지켜지지 않는 경향이 있습니다. 그 이유는 여러 견해가 있으나, 가장 기본적인 견해는 한국어의 경우 띄어쓰기가 지켜지지 않아도 글을 쉽게 이해할 수 있는 언어라는 점입니다. 사실, 띄어쓰기가 없던 한국어에 띄어쓰기가 보편화된 것도 근대(1933년, 한글맞춤법통일안)의 일입니다.\n",
        "\n",
        "\n",
        "# 품사 태깅(Part-of-Speech tagging)\n",
        "\n",
        "\n",
        "단어는 표기는 같지만, 품사에 따라서 단어의 의미가 달라지기도 합니다. 예를 들어서 영어 단어 'fly'는 동사로는 '날다'라는 의미를 갖지만, 명사로는 '파리'라는 의미를 갖고있습니다. 한국어도 마찬가지입니다. '못'이라는 단어는 명사로서는 망치를 사용해서 목재 따위를 고정하는 물건을 의미합니다. 하지만 부사로서의 '못'은 '먹는다', '달린다'와 같은 동작 동사를 할 수 없다는 의미로 쓰입니다. 즉, 결국 단어의 의미를 제대로 파악하기 위해서는 해당 단어가 어떤 품사로 쓰였는지 보는 것이 주요 지표가 될 수도 있습니다. 그에 따라 단어 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지를 구분해놓기도 하는데, 이 작업을 품사 태깅(part-of-speech tagging)이라고 합니다. NLTK에서는 어떻게 품사 태깅이 되는지 실습을 통해서 알아보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPp_IMdCN70c",
        "outputId": "a318e13b-b5bd-4730-ef10-dbb7817f76a3"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpzjYdZuAB2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceb06035-4242-495c-f408-089a67df5566"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "print(word_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVFNXzVoNz0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1456abe-1ab6-4f46-ae75-b700ee5a0700"
      },
      "source": [
        "from nltk.tag import pos_tag\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "x=word_tokenize(text)\n",
        "pos_tag(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('am', 'VBP'),\n",
              " ('actively', 'RB'),\n",
              " ('looking', 'VBG'),\n",
              " ('for', 'IN'),\n",
              " ('Ph.D.', 'NNP'),\n",
              " ('students', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('and', 'CC'),\n",
              " ('you', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('a', 'DT'),\n",
              " ('Ph.D.', 'NNP'),\n",
              " ('student', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5cGomarZpPH"
      },
      "source": [
        "- 영어 문장에 대해서 토큰화를 수행하고, 이어서 품사 태깅을 수행하였습니다. Penn Treebank POG Tags에서 PRP는 인칭 대명사, VBP는 동사, RB는 부사, VBG는 현재부사, IN은 전치사, NNP는 고유 명사, NNS는 복수형 명사, CC는 접속사, DT는 관사를 의미합니다.\n",
        "\n",
        "- 한국어 자연어 처리를 위해서는 KoNLPy(\"코엔엘파이\"라고 읽습니다)라는 파이썬 패키지를 사용할 수 있습니다. 코엔엘파이를 통해서 사용할 수 있는 형태소 분석기로 Okt(Open Korea Text), 메캅(Mecab), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma)가 있습니다.\n",
        "\n",
        "- 한국어 NLP에서 형태소 분석기를 사용한다는 것은 단어 토큰화가 아니라 정확히는 형태소(morpheme) 단위로 형태소 토큰화(morpheme tokenization)를 수행하게 됨을 뜻합니다. 여기선 이 중에서 Okt와 꼬꼬마를 통해서 토큰화를 수행해보도록 하겠습니다. (Okt는 기존에는 Twitter라는 이름을 갖고있었으나 0.5.0 버전부터 이름이 변경되어 인터넷에는 아직 Twitter로 많이 알려져있으므로 학습 시 참고바랍니다.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u780Vl06aFA9",
        "outputId": "a8605cc3-fa76-454e-f101-0a13743ffe60"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 7.8MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 43.8MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: beautifulsoup4, JPype1, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLh3AkQvQfA2",
        "outputId": "0764fe0f-d007-4b73-9391-e1fce157470a"
      },
      "source": [
        "from konlpy.tag import Okt  \n",
        "okt=Okt()  \n",
        "print(okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
        "# 텍ㅌ스트를 형태로 단위로 나누는데,이때 각 단어에서 어간을 추출"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOOsZK_baEQ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48bbde0a-d42b-40ed-9b6f-78f56c7ac0b4"
      },
      "source": [
        "print(okt.pos('열심히 코딩한 당신, 연휴에는 여행을 가봐요'))\n",
        "# 품사추출."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-_ERx0nfG56"
      },
      "source": [
        "1) morphs : 형태소 추출\n",
        "\n",
        "2) pos : 품사 태깅\n",
        "\n",
        "3) nouns : 명사 추출\n",
        "\n",
        "\n",
        "이번에는 꼬꼬마 형태소 분석기를 사용하여 같은 문장에 대해서 토큰화를 진행해 보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DXkiL4AfRp3",
        "outputId": "b4591367-01ec-4cd5-86ed-38f7b69b3e06"
      },
      "source": [
        "from konlpy.tag import Kkma  \n",
        "kkma=Kkma()  \n",
        "print(kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oDxkMfvfVQd",
        "outputId": "f99365af-726b-499e-9d60-0f6a576f2186"
      },
      "source": [
        "print(kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUe1AGHVfbI9"
      },
      "source": [
        "앞서 사용한 Okt 형태소 분석기와 결과가 다른 것을 볼 수 있습니다. 각 형태소 분석기는 성능과 결과가 다르게 나오기 때문에, 형태소 분석기의 선택은 사용하고자 하는 필요 용도에 어떤 형태소 분석기가 가장 적절한지를 판단하고 사용하면 됩니다. 예를 들어서 속도를 중시한다면 메캅을 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1ijHA6nffB9"
      },
      "source": [
        "# 2) 정제(Clearning) and 정규화(Normalization)\n",
        "\n",
        "코퍼스에서 용도에 맞게 토큰을 분류하는 작업을 토큰화(tokenization)라고 하며, 토큰화 작업 전, 후에는 텍스트 데이터를 용도에 맞게 정제(cleaning) 및 정규화(normalization)하는 일이 항상 함께합니다\n",
        "\n",
        " - 정제(Clearning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.\n",
        " - 정규화(Normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어 줌.\n",
        "\n",
        "\n",
        " 정제 작업은 토큰화 작업에 방해가 되는 부분들을 배제시키고 토큰화 작업을 수행하기 위해서 토큰화 작업보다 앞서 이루어지기도 하지만, 토큰화 작업 이후에도 여전히 남아있는 노이즈들을 제거하기위해 지속적으로 이루어지기도 합니다. 사실 완벽한 정제 작업은 어려운 편이라서, 대부분의 경우 이 정도면 됐다.라는 일종의 합의점을 찾기도 합니다.\n",
        "\n",
        "# 1. 규칙에 기반한 표기가 다른 단어들의 통합.\n",
        " - USA와 US는 같은 의미를 가지므로, 하나의 단어로 정규화해볼 수 있습니다. uh-huh와 uhhuh는 형태는 다르지만 여전히 같은 의미를 갖고 있습니다. 이러한 정규화를 거치게 되면, US를 찾아도 USA도 함께 찾을 수 있을 것입니다. \n",
        "\n",
        "# 2. 대, 소문자 통합\n",
        " - 소문자 변환이 왜 유용한지 예를 들어보도록 하겠습니다. 가령, Automobile이라는 단어가 문장의 첫 단어였기때문에 A가 대문자였다고 생각해봅시다. 여기에 소문자 변환을 사용하면, automobile을 찾는 질의(query)에서, 결과로서 Automobile도 찾을 수 있게 됩니다.\n",
        "\n",
        " - 이러한 작업은 더 많은 변수를 사용해서 소문자 변환을 언제 사용할지 결정하는 머신 러닝 시퀀스 모델로 더 정확하게 진행시킬 수 있습니다. 하지만 만약 올바른 대문자 단어를 얻고 싶은 상황에서, 훈련에 사용하는 코퍼스가 사용자들이 단어의 대문자, 소문자의 올바른 사용 방법과 상관없이 소문자를 사용하는 사람들로부터 나온 데이터라면 이러한 방법 또한 그다지 도움이 되지 않을 수 있습니다. 결국에는 예외 사항을 크게 고려하지 않고, 모든 코퍼스를 소문자로 바꾸는 것이 종종 더 실용적인 해결책이 되기도 합니다.\n",
        "\n",
        "# 3. 불필요한 단어의 제거(Removing Unnecessary Words)\n",
        "\n",
        "- 정제 작업에서 제거해야하는 노이즈 데이터(noise data)는 자연어가 아니면서 아무 의미도 갖지 않는 글자들(특수 문자 등)을 의미하기도 하지만, 분석하고자 하는 목적에 맞지 않는 불필요 단어들을 노이즈 데이터라고 하기도 함.\n",
        "\n",
        "1) 등장 빈도가 적은 단어(Removing Rare Words)\n",
        "- 텍스트 데이터에서 너무 적게 등장해서 자연어 처리에 도움이 되지 않는 단어들이 존재함\n",
        "\n",
        "ex) 총 100,000개개의 메일을 가지고 정상 메일을 판단하는 모델을 설계하려고 하는데 특정 단어가 5번 미만으로 등장하면 분류에 도움이 되지 않음.\n",
        "\n",
        "2) 길이가 짧은 단어(Removin words with very a short lenghth)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpjvokC7fYcz",
        "outputId": "edb7576c-e091-47bb-aa69-a93d2bda8a23"
      },
      "source": [
        "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
        "import re\n",
        "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
        "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "print(shortword.sub('', text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " was wondering anyone out there could enlighten this car.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35mx566Aj7-j"
      },
      "source": [
        "4. 정규표현식(Regular Expression)\n",
        "-  HTML 문서로부터 가져온 코퍼스라면 문서 여기저기에 HTML 태그가 있습니다. 뉴스 기사를 크롤링 했다면, 기사마다 게재 시간이 적혀져 있을 수 있습니다. 정규 표현식은 이러한 코퍼스 내에 계속해서 등장하는 글자들을 규칙에 기반하여 한 번에 제거하는 방식으로서 매우 유용합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTF-AzJV6K1P"
      },
      "source": [
        "## Stemming(어간 추출)과 Lemmatization(표제어 추출)\n",
        "\n",
        "- 표제어(lemma)는 한글로는 '표제어'또는 '기본 사전형 단어' 정도의 의미를 갖음. 표제어 추출은 단어들로부터 표제어를 찾아가는 과정. 표제어 추출은 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단.\n",
        "\n",
        "-예를 들어 am,are,is는 서로 다른 스펠링이지만, 그 뿌리 단어는 be라고 볼 수 있음. 이때, 이 단어들의 표제어는 be라고 함.\n",
        "\n",
        "-표제어 추출을 하는 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행하는 것. 형태소란 '의미를 가진 가장 작은 단위'뜻함.\n",
        "\n",
        "- 형태소는 두 가지 종류가 있음. 각각 어간(stem)과 접사(affix)입니다.\n",
        "\n",
        "1. 어간(stem) : 단어의 의미를 담고 있는 단어의 핵심 부분\n",
        "2. 접사(affix) : 단어에 추가적인 의미를 주는 부분\n",
        "\n",
        "- 형태학적 파싱은 이 두가지 구성 요소를 분리하는 작업. \n",
        "\n",
        "- 가령, cats라는 단어에 대해 형태학적 파싱을 수행한다면, 형태학적 파싱은 결과로 cat(어간)와 -s(접사)를 분리. 꼭 두가지로 분리되지 않는 경우도 있음. 단어 fox는 형태학적 파싱을 한다고 하더라도 분리할 수 없음."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex6hDBU96JLr",
        "outputId": "77ca1c20-60b8-4ab9-be5f-fdba0e1eb522"
      },
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
        "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "happy happiest\n",
            "fant fanciest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spBqjCXz6MuA",
        "outputId": "116c9723-1c1d-4f13-9f2f-b1abbac4cc0f"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
        "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
        "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "amuse amuse amuse\n",
            "happy happy\n",
            "fancy fancy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dykrXx6pwnKU",
        "outputId": "ca6f41a4-c207-41ae-a875-9ce991f9abfe"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "n=WordNetLemmatizer()\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([n.lemmatize(w) for w in words])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf4rawtmwrlj"
      },
      "source": [
        "잠시 후 어간 추출(stemming)에 대해서 배우고, 같은 입력에 대한 결과를 비교해보시면 알겠지만 표제어 추출은 어간 추출과는 달리 단어의 형태가 적절히 보존되는 양상을 보이는 특징이 있습니다. 하지만 그럼에도 위의 결과에서는 dy나 ha와 같이 의미를 알 수 없는 적절하지 못한 단어를 출력하고 있습니다. 이는 표제어 추출기(lemmatizer)가 본래 단어의 품사 정보를 알아야만 정확한 결과를 얻을 수 있기 때문입니다.\n",
        "\n",
        "WordNetLemmatizer는 입력으로 단어가 동사 품사라는 사실을 알려줄 수 있습니다. 즉, dies와 watched, has가 문장에서 동사로 쓰였다는 것을 알려준다면 표제어 추출기는 품사의 정보를 보존하면서 정확한 Lemma를 출력하게 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tRs_GZmDwqbt",
        "outputId": "1918f684-27f1-4e10-bde6-4b16edfe27f3"
      },
      "source": [
        "n.lemmatize('dies', 'v')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'die'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0svw7mmDwu8a",
        "outputId": "30b0242b-af50-4fff-8023-b4fcd9541d04"
      },
      "source": [
        "n.lemmatize('watched', 'v')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'watch'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yI8AB3XjwxBq",
        "outputId": "7461830a-5899-45e6-8a08-a826b042dbf1"
      },
      "source": [
        "n.lemmatize('has', 'v')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'have'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdilv8qOlgDd"
      },
      "source": [
        "# 어간 추출(Stemming)\n",
        "\n",
        "- 어간(Stem)을 추출하는 작업을 어간 추출(stemming)이라고 합니다. 어간 추출은 형태학적 분석을 단순화한 버전이라고 볼 수도 있고, 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라고 볼 수도 있습니다. 다시 말해, 이 작업은 섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYEvoFIxj6q3",
        "outputId": "c0763b19-5b35-4cd5-89f4-bbd16676fa71"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "s = PorterStemmer()\n",
        "text=\"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "words=word_tokenize(text)\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf3ERyVelqku",
        "outputId": "ec87537f-5525-4edc-8be2-52581fdeb36e"
      },
      "source": [
        "print([s.stem(w) for w in words])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAQXJ6_Tlsq5"
      },
      "source": [
        "위의 알고리즘의 결과에는 사전에 없는 단어들도 포함되어 있습니다. 위의 어간 추출은 단순 규칙에 기반하여 이루어지기 때문입니다.\n",
        "\n",
        "가령, 포터 알고리즘의 어간 추출은 이러한 규칙들을 가집니다.\n",
        "\n",
        "ALIZE → AL\n",
        "\n",
        "ANCE → 제거\n",
        "\n",
        "ICAL → IC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBH9wDY8lvz7",
        "outputId": "8182a874-7e44-4df7-d81b-9c0e8e7df761"
      },
      "source": [
        "words=['formalize', 'allowance', 'electricical']\n",
        "print([s.stem(w) for w in words])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['formal', 'allow', 'electric']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3MBpIPRly3c"
      },
      "source": [
        "위의 규칙에 따라서 다음과 같은 결과가 나옵니다.\n",
        "\n",
        "formalize → formal\n",
        "\n",
        "allowance → allow\n",
        "\n",
        "electricical → electric\n",
        "\n",
        "어간 추출 속도는 표제어 추출보다 일반적으로 빠른데, 포터 어간 추출기는 정밀하게 설계되어 정확도가 높으므로 영어 자연어 처리에서 어간 추출을 하고자 한다면 가장 준수한 선택입니다. NLTK에서는 포터 알고리즘 외에도 랭커스터 스태머(Lancaster Stemmer) 알고리즘을 지원합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EsKFMwOlwM3",
        "outputId": "ceda3649-c57c-473b-ac56-d6659f49a2c0"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "s=PorterStemmer()\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([s.stem(w) for w in words])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X1eiQe4lwPf",
        "outputId": "d20284d2-a9a4-4cf8-bc04-2be14d2aeaa1"
      },
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "l=LancasterStemmer()\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([l.stem(w) for w in words])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A55t7Xa9l-h1"
      },
      "source": [
        "동일한 단어들의 나열에 대해서 두 스태머는 전혀 다른 결과를 보여줍니다. 두 스태머 알고리즘은 서로 다른 알고리즘을 사용하기 때문입니다. 그렇기 때문에 이미 알려진 알고리즘을 사용할 때는, 사용하고자 하는 코퍼스에 스태머를 적용해보고 어떤 스태머가 해당 코퍼스에 적합한지를 판단한 후에 사용하여야 합니다.\n",
        "\n",
        "## 한국어에서의 어간 추출\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWXeolOW3wKo"
      },
      "source": [
        "# 불용어(Stopwords) 제거\n",
        "\n",
        "갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요합니다. 여기서 큰 의미가 없다라는 것은 자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 단어들을 말합니다. 예를 들면, I, my, me, over, 조사, 접미사 같은 단어들은 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 거의 기여하는 바가 없는 경우가 있습니다. 이러한 단어들을 불용어(stopword)라고 하며, NLTK에서는 위와 같은 100여개 이상의 영어 단어들을 불용어로 패키지 내에서 미리 정의하고 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmvmcDWI3u_x",
        "outputId": "e0b39dba-8d42-4b0c-9cf5-d0489360decb"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoWylmJV3zY4",
        "outputId": "7fa34a2c-3c43-4c51-d0ca-b9a3ab05d04f"
      },
      "source": [
        "print('영어 stop words 갯수:',len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "영어 stop words 갯수: 179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz_VLM8V33Co",
        "outputId": "0d8f17c0-e38c-433e-9ba5-356072c727f6"
      },
      "source": [
        "import nltk\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "# 위 예제의 3개의 문장별로 얻은 word_tokens list 에 대해 stop word 제거 Loop\n",
        "for sentence in word_tokens:\n",
        "    filtered_words=[]\n",
        "    # 개별 문장별로 tokenize된 sentence list에 대해 stop word 제거 Loop\n",
        "    for word in sentence:\n",
        "        #소문자로 모두 변환합니다. \n",
        "        word = word.lower()\n",
        "        # tokenize 된 개별 word가 stop words 들의 단어에 포함되지 않으면 word_tokens에 추가\n",
        "        if word not in stopwords:\n",
        "            filtered_words.append(word)\n",
        "    all_tokens.append(filtered_words)\n",
        "    \n",
        "print(all_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KEdiA8dmato"
      },
      "source": [
        "- 한국어에서 불용어 제거하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYUTBGE7lwSr",
        "outputId": "d65d335a-421c-4991-a0fa-8cafe5d46821"
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
        "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든\"\n",
        "# 위의 불용어는 명사가 아닌 단어 중에서 저자가 임의로 선정한 것으로 실제 의미있는 선정 기준이 아님\n",
        "stop_words=stop_words.split(' ')\n",
        "word_tokens = word_tokenize(example)\n",
        "\n",
        "result = [] \n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        result.append(w) \n",
        "# 위의 4줄은 아래의 한 줄로 대체 가능\n",
        "# result=[word for word in word_tokens if not word in stop_words]\n",
        "\n",
        "print(word_tokens) \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n",
            "['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은', '게', '.', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3q068w6mjnX"
      },
      "source": [
        "# 정규 표현식(Regular Expression)\n",
        "\n",
        "1)정규 표현식 문법\n",
        "\n",
        "\n",
        "<figure>\n",
        "<img src = 'https://miro.medium.com/max/700/1*Y-q0dkUClSW0dX6uuysnJQ.png'>\n",
        "<figure>\n",
        "\n",
        "<figure>\n",
        "<img src = 'https://miro.medium.com/max/700/1*c__WeRlFyGY_-7LeKQkNnQ.png'>\n",
        "<figure>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJdmfWnvnXkU"
      },
      "source": [
        "<figure>\n",
        "<img src = 'https://miro.medium.com/max/661/1*6pIp6zuIoRHUOXjEDDUNCA.png'>\n",
        "<figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOJwqo3YnoPW"
      },
      "source": [
        "## 정규 표현식 실습\n",
        "\n",
        "1) .은 한 개의 임의의 문자를 나타낸다. 예를 들어 정규 표현식이 a.c라고 한다면, a와 c사이에 어떤 1개의 문자라도 올 수 있다는 뜻이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWAKfa_XlwVs",
        "outputId": "e8607d00-0d34-447f-e5d6-2035206e7bc6"
      },
      "source": [
        "import re\n",
        "r = re.compile(\"a.c\")\n",
        "r.search(\"kkk\")\n",
        "# 아무런 결과도 출력되지 않는다.\n",
        "r.search(\"abc\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrXq0RyVnvvE"
      },
      "source": [
        "2) ?\n",
        "\n",
        "?는 ? 앞의 문자가 존재할 수도 있고, 존재하지 않을 수도 있는 경우를 나타낸다. 예를 들어서 정규 표현식이 ab?c라고 한다면, b는 있다고 취급할 수도 있고 없다고 취급할 수도 있다. 즉, abc와 ac 모두 매치할 수 있다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucM6z0vbnvGQ",
        "outputId": "89a8a7c6-9606-4692-8608-ac24c678ee14"
      },
      "source": [
        "r = re.compile(\"ab?c\")\n",
        "r.search(\"abbc\")\n",
        "# 아무것도 출력되지 않음\n",
        "r.search(\"abc\")\n",
        "# b가 있는 것으로 판단하여 abc를 매치함."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEBFlSaJn0aQ",
        "outputId": "1cb5f026-deed-4228-bc20-b6ac461baa30"
      },
      "source": [
        "r.search(\"ac\")\n",
        "# b가 없는 것으로 판단하여 ac를 매치함."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 2), match='ac'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFr7rlCUn6BM"
      },
      "source": [
        "3) * 기호\n",
        "\n",
        "\n",
        "*은 바로 앞의 문자가 0개 이상일 경우를 나타낸다. 앞의 문자는 존재하지 않을 수도 있으며, 또는 여러 개일 수도 있다. 예를 들어 정규 표현식이 abc라고 한다면 ac, abc, abbc, abbbc 등과 매치할 수 있다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CiNe8Fpn5L4"
      },
      "source": [
        "r = re.compile(\"ab*c\")\n",
        "r.search(\"a\")\n",
        "# 아무것도 출력되지 않음."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q13_T3don5OR",
        "outputId": "84cd714d-9b49-433f-fa50-6a0d0ee1c69b"
      },
      "source": [
        "r.search(\"ac\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 2), match='ac'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZB6GP8zn5SU",
        "outputId": "fd4bbcc1-1a1c-498f-c665-693173f5a6c1"
      },
      "source": [
        "r.search(\"abc\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVGmeToLn5Vb",
        "outputId": "1132c7fe-a8ba-42d8-f825-ba22ee6b18dc"
      },
      "source": [
        "r.search(\"abbc\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 4), match='abbc'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSkU7IyWoA1h"
      },
      "source": [
        "4) + 기호\n",
        "\n",
        "+는 *와 유사하다. 하지만 다른 점은 앞의 문자가 최소 1개 이상이어야 한다. 예를 들어 ab+c라고 하면, ac는 매치되지 않는다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErjEqSScn5ZN",
        "outputId": "800204fd-c1fa-4a7b-a336-b8f642964abc"
      },
      "source": [
        "r = re.compile('ab+c')\n",
        "r.search(\"ac\")\n",
        "# 아무것도 출력되지 않는다.\n",
        "\n",
        "r.search(\"abc\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UecA1Y3OoGlE"
      },
      "source": [
        "5) ^ 기호\n",
        "\n",
        "^는 시작되는 글자를 지정한다. 가령 정규표현식이 ^a라면 a로 시작되는 문자열만을 찾아낸다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5WNrSLUn5cB",
        "outputId": "3fd3219d-e5eb-404c-bd4e-f434ee0e9fc6"
      },
      "source": [
        "r = re.compile('^a')\n",
        "r.search('bbc')\n",
        "# 아무것도 출력되지 않음\n",
        "r.search('ab')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='a'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqVYqmPdoKs9"
      },
      "source": [
        "6) {숫자} 기호\n",
        "\n",
        "문자에 해당 기호를 붙이면, 해당 문자를 숫자만큼 반복한 것을 나타낸다. 예를 들어 정규 표현식이 ab{2}c라면 a와 c사이에 b가 존재하면서 b가 2개인 문자열에 대해서 매치한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOqSSzqVn5eh",
        "outputId": "140d5cb1-06e4-4091-e24e-062f7d2bd7cb"
      },
      "source": [
        "r = re.compile(\"ab{2}c\")\n",
        "r.search(\"ac\")\n",
        "# 아무것도 출력되지 않는다.\n",
        "r.search(\"abc\")\n",
        "# 아무것도 출력되지 않는다.\n",
        "r.search(\"abbc\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 4), match='abbc'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rKFOAa-n5iT"
      },
      "source": [
        "r.search(\"abbbbbc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZcFPFygoZa7"
      },
      "source": [
        "7) {숫자1, 숫자2} 기호\n",
        "\n",
        "문자에 해당 기호를 붙이면, 해당 문자를 숫자1 이상 숫자2 이하만큼 반복한다. 예를 들어 정규 표현식이 ab{2,8}c라면 a와 c사이에 b가 존재하면서 b는 2개 이상 8개 이하인 문자열에 대해서 매치한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dETY6f16oQ4N",
        "outputId": "95eb4525-a008-4789-c386-5c4e14e5236b"
      },
      "source": [
        "r = re.compile(\"ab{2,8}c\")\n",
        "r.search('ac')\n",
        "# 아무런 결과도 출력되지 않음\n",
        "r.search(\"abc\")\n",
        "# 아무런 결과도 출력되지 않음\n",
        "r.search(\"abbc\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 4), match='abbc'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSy5xqYkoQ63",
        "outputId": "80fd92b4-38d3-4167-b79a-f9cba9cd40cf"
      },
      "source": [
        "r.search(\"abbbbc\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 6), match='abbbbc'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYt_Z16NoQ9l"
      },
      "source": [
        "r.search(\"abbbbbbbbbbbbbc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MphGl8jQoht6"
      },
      "source": [
        "8) {숫자,} 기호\n",
        "\n",
        "문자에 해당 기호를 붙이면 해당 문자를 숫자 이상만큼 반복한다. 예를 들어 정규 표현식이 a{2,}bc라면 뒤에 bc가 붙으면서 a의 갯수가 2개 이상인 경우인 문자열과 매치한다. 또한 만약 {0,}을 쓴다면 *와 동일한 의미가 되며, {1,}을 쓴다면 +와 동일한 의미가 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYDNKfksoRAM",
        "outputId": "6f92d058-b1ca-4332-8263-c6cec2f8fed1"
      },
      "source": [
        "r = re.compile(\"a{2,}bc\")\n",
        "r.search(\"bc\")\n",
        "# 아무런 결과도 출력되지 않음\n",
        "r.search(\"aa\")\n",
        "# 아무런 결과도 출력되지 않음\n",
        "r.search(\"aabc\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 4), match='aabc'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMS5yC4PoRC2",
        "outputId": "38c12346-717a-43ac-ea53-f3b90b91c569"
      },
      "source": [
        "r.search(\"aaaaabc\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 7), match='aaaaabc'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95LIGxFfopJZ"
      },
      "source": [
        "9) [] 기호\n",
        "\n",
        "[]안에 문자들을 넣으면 그 문자들 중 한 개의 문자와 매치라는 의미를 가진다. 예를 들어 정규 표현식이 [abc]라면, a 또는 b 또는 c가 들어가 있는 문자열과 매치된다. 범위를 지정하는 것도 가능하다. [a-zA-Z]는 알파벳 전부를 의미하며, [0–9]는 숫자 전부를 의미한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV1ubrIcoRFe",
        "outputId": "bbc3e078-9f5b-484b-ffba-83645012b5eb"
      },
      "source": [
        "r = re.compile(\"[abc]\")\n",
        "r.search('zzz')\n",
        "# 아무것도 출력되지 않음\n",
        "r.search('a')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='a'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VdzaHtYoRJy",
        "outputId": "110ad99b-f9b0-4f78-d4f4-25a317f9c335"
      },
      "source": [
        "r.search('acg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='a'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj_txezzot9l",
        "outputId": "a14fd5b1-c920-4856-8d88-9524c7ccb5c5"
      },
      "source": [
        "r.search('babo')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='b'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umeVgS7Uownl"
      },
      "source": [
        "이번에는 알파벳 소문자에 대해서만 범위를 지정하여 정규표현식을 만들어보고 문자열과 매치한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE4DTmOyouBM",
        "outputId": "e635b2eb-0e4a-4714-97fe-311536e1afca"
      },
      "source": [
        "r = re.compile(\"[a-z]\")\n",
        "r.search('AAA')\n",
        "# 아무것도 출력되지 않음\n",
        "r.search('aA')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='a'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwDQcJEPpAMv"
      },
      "source": [
        "10) [^문자]기호\n",
        "\n",
        "[^문자]는 5)에서 설명한 ^와는 완전히 다른 의미로 쓰인다. 여기서는 ^ 기호 뒤에 붙은 문자들을 제외한 모든 문자를 매치하는 역할을 한다. 예를 들어 [^abc]라는 정규 표현식이 있다면, a 또는 b 또는 c가 들어간 문자열을 제외한 모든 문자열을 매치한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6khEjAygouEM",
        "outputId": "83908e95-d682-4da3-8fc6-1766d1f802f3"
      },
      "source": [
        "r = re.compile('[^abc]')\n",
        "r.search(\"a\")\n",
        "# 아무것도 출력되지 않음\n",
        "r.search(\"ahoho\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(1, 2), match='h'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIZKaAHpouIH",
        "outputId": "9eb93481-1033-4763-9298-f5f926d9784a"
      },
      "source": [
        "r.search(\"1st\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 1), match='1'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s24N9GSQpHiP"
      },
      "source": [
        "# 정규 표현식 모듈 함수 예제\n",
        "\n",
        "re.compile()과 re.search() 이외에도 다른 여러 정규 표현식 모듈 함수에 대해서 알아보자.\n",
        "\n",
        "\n",
        "### (1) re.match()와 re.search()의 차이\n",
        "\n",
        "search()가 정규 표현식 전체에 대해서 문자열이 매치하는지를 본다면, match()는 문자열의 첫 부분부터 정규표현식과 매치하는지를 확인한다. 문자열 중간에 찾을 패턴이 있다고 하더라도, match 함수는 문자열의 시작에서 패턴이 일치하지 않으면 찾지 않는다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jo7NJ8FFpGkx",
        "outputId": "9fac4d2f-4d42-4a2b-f326-3f9525349fcc"
      },
      "source": [
        "r = re.compile(\"ab.\")\n",
        "r.search(\"kkkabc\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(3, 6), match='abc'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F26A2vvVpUTP"
      },
      "source": [
        "r.match(\"kkkabc\")\n",
        "# 아무것도 출력되지 않음"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JOMnM1QpW7D",
        "outputId": "854cbbee-e9d0-40ad-df42-74d3e2980094"
      },
      "source": [
        "r.match(\"abckkk\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON2l63kepQ9c"
      },
      "source": [
        "위의 경우 ab.이기 때문에 ab뒤에 어떤 한 글자가 존재할 수 있다는 패턴을 의미한다. search 모듈 함수에 kkkabc라는 문자열을 넣어 매치되는지 확인한다면 abc라는 문자열에서 매치되어 Match object를 리턴한다. 하지만 match 모듈 함수의 경우 앞 부분이 ab.와 매치되지 않기 때문에, 아무런 결과도 출력하지 않는다. 하지만 abckkk로 시도하면 시작 부분과 매치되었기 때문에 정상적으로 Match object를 리턴한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xsVXDAvpZVq"
      },
      "source": [
        "### (2) re.split()\n",
        "split() 함수는 입력된 정규 표현식을 기준으로 문자열들을 분리하여 리스트로 리턴한다. 자연어 처리에 있어서 가장 많이 사용되는 정규 표현식 함수 중 하나인데, 토큰화에 유용하게 쓰일 수 있기 때문이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBqfSX0UpGn9",
        "outputId": "166309ce-6210-4a30-a67c-794a31708505"
      },
      "source": [
        "text = \"사과 딸기 수박 멜론 바나나\"\n",
        "re.split(\" \",text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['사과', '딸기', '수박', '멜론', '바나나']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgBvcTRPpGt8",
        "outputId": "031ae5a2-60b9-4ce8-b950-40b84af9322a"
      },
      "source": [
        "text = \"\"\"사과\n",
        "딸기\n",
        "수박\n",
        "멜론\n",
        "바나나\"\"\"\n",
        "re.split(\"\\n\", text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['사과', '딸기', '수박', '멜론', '바나나']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnQw92xFpG0I",
        "outputId": "c0b0317b-676e-4f2d-bc82-f3557b6657d3"
      },
      "source": [
        "text = \"사과+딸기+수박+멜론+바나나\"\n",
        "re.split(\"\\+\", text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['사과', '딸기', '수박', '멜론', '바나나']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg2aGh9Wpfoi"
      },
      "source": [
        "### (3) re.findall()\n",
        "\n",
        "findall() 함수는 정규 표현식과 매치되는 모든 문자열들을 리스트로 리턴한다. 단, 매치되는 문자열이 없다면 빈 리스트를 리턴한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeYg7WNnpG4K",
        "outputId": "71668842-b528-43cd-8150-509cac0597c3"
      },
      "source": [
        "text = \"\"\"이름 : 김철수\n",
        "전화번호 : 010 - 1234 - 1234\n",
        "나이 : 30\n",
        "성별 : 남\"\"\"\n",
        "re.findall(\"\\d+\", text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['010', '1234', '1234', '30']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJKGJOyhpG7p",
        "outputId": "496a5c65-d7f5-48ae-c57a-8539d31dae97"
      },
      "source": [
        "re.findall(\"\\d+\", \"문자열입니당.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vh9SKY2pkls"
      },
      "source": [
        "[]  # 빈 리스트를 리턴했다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMvuNMA9pmDz"
      },
      "source": [
        "### (4) re.sub()\n",
        "sub() 함수는 정규 표현식 패턴과 일치하는 문자열을 찾아 다른 문자열로 대체할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "agOy_pS6pG_6",
        "outputId": "b01c6ece-5379-4be6-d9e9-ac2f4db9804f"
      },
      "source": [
        "text=\"Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern.\"\n",
        "re.sub('[^a-zA-Z]',' ',text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Regular expression   A regular expression  regex or regexp     sometimes called a rational expression        is  in theoretical computer science and formal language theory  a sequence of characters that define a search pattern '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ypWDGeuptnH"
      },
      "source": [
        "위와 같은 경우, 영어 문장에 각주 등과 같은 이유로 특수 문자가 섞여 있다. 자연어 처리를 위해 특수 문자를 제거하고 싶다면 알파벳 외의 문자는 공백으로 처리하는 등의 사용 용도로 쓸 수 있다.\n",
        "\n",
        "\n",
        "##5. 정규 표현식 텍스트 전처리 예제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "werXP728pwbr",
        "outputId": "c3e3c117-dd08-4fed-81ca-0963207fb7e1"
      },
      "source": [
        "text = \"\"\"100 John    PROF\n",
        "101 James    STUD\n",
        "102 Mac    STUD\"\"\"\n",
        "re.split('\\s+', text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['100', 'John', 'PROF', '101', 'James', 'STUD', '102', 'Mac', 'STUD']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wbXUkKRpzg-"
      },
      "source": [
        "'\\s+' 는 공백 여러 개를 기준으로 split한다는 의미."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW5hRr0jpwsB",
        "outputId": "88f0ed35-6e22-499d-c981-409863cc2dc0"
      },
      "source": [
        "re.findall('\\d+', text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['100', '101', '102']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2zf6Da5p3m8"
      },
      "source": [
        "'\\d+' 숫자로 이루어진 문자열을 리스트로 반환."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzITueLxp480",
        "outputId": "9bfd7dc9-3d0c-4b30-f494-8980285640c8"
      },
      "source": [
        "re.findall('[A-Z]', text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['J', 'P', 'R', 'O', 'F', 'J', 'S', 'T', 'U', 'D', 'M', 'S', 'T', 'U', 'D']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYIgh8kqp64x"
      },
      "source": [
        "대문자에 해당하는 문자를 리스트로 반환. 하지만 PROF, STUD 등을 알고 싶다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ5NciT0pwvz",
        "outputId": "129c00be-5f38-4101-e606-92a29b39ff0c"
      },
      "source": [
        "re.findall('[A-Z]{4}',text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PROF', 'STUD', 'STUD']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qh4Bddxvpwz3",
        "outputId": "d373fd3f-c8bd-4e86-ebe6-dc326badbd16"
      },
      "source": [
        "re.findall('[A-Z][a-z]+', text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['John', 'James', 'Mac']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k6w4OdBp--3"
      },
      "source": [
        "대문자로 시작하고, 소문자가 이어지는 문자열을 반환 (이름)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_5pEyVeJpw28",
        "outputId": "6e014519-16a8-495e-8534-1973a53205dd"
      },
      "source": [
        "letters_only = re.sub('[^a-zA-Z]', ' ', text)\n",
        "letters_only"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'    John    PROF     James    STUD     Mac    STUD'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6I-N1xlqCLg"
      },
      "source": [
        "영문자가 아닌 문자는 전부 공백으로 치환\n",
        "\n",
        "### 정규 표현식을 이용한 토큰화\n",
        "NLTK에서는 정규 표현식을 사용해서 단어 토큰화를 수행하는 RegexpTokenizer를 지원한다. RegexpTokenizer()에서 괄호 안에 원하는 정규 표현식을 넣어서 토큰화를 수행하는 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSoqN-2Lpw6I",
        "outputId": "b65f3663-4304-4802-bdc5-57fe62cca367"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
        "print(tokenizer.tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT6elS-DqL72"
      },
      "source": [
        "\\w+ 는 문자 또는 숫자가 1개 이상인 경우를 인식하는 코드이다. 그렇기 때문에 이 코드는 문장에서 구두점을 제외하고, 단어들만 가지고 토큰화를 수행한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxESbVBjpw9T",
        "outputId": "dc51c66f-6f14-43d2-b633-b11fbd98a6b9"
      },
      "source": [
        "tokenizer = RegexpTokenizer(\"[\\s]+\", gaps=True)\n",
        "print(tokenizer.tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', 'Mr.', \"Jone's\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4XSHr9uqPTN"
      },
      "source": [
        "위 코드에서 gaps =True 는 해당 정규 표현식을 토큰으로 나누기 위한 기준으로 사용한다는 의미다. 따라서 \\s+ 는 공백을 의미하므로 공백을 기준으로 토큰화된 결과값을 출력한다. gaps=True를 설정하지 않으면 공백만이 출력된다.(공백으로 시작하는 것들을 찾음) 그 전 결과와 비교한다면 어퍼스트로피나 온점을 제외하지 않고, 토큰화가 수행된 것을 확인할 수 있다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I60Nr2JiqapF"
      },
      "source": [
        "# 정수 인코딩(inter Encoding)\n",
        "\n",
        "- 각 단어를 고유한 정수에 맵핑(mapping)시키는 전처리 작업이 필요할 때가 있습니다.\n",
        "\n",
        "\n",
        "예를 들어 갖고 있는 텍스트에 단어가 5,000개가 있다면, 5,000개의 단어들 각각에 1번부터 5,000번까지 단어와 맵핑되는 고유한 정수, 다른 표현으로는 인덱스를 부여합니다. 가령, book은 150번, dog는 171번, love는 192번, books는 212번과 같이 숫자가 부여됩니다. 인덱스를 부여하는 방법은 여러 가지가 있을 수 있는데 랜덤으로 부여하기도 하지만, 보통은 전처리 또는 빈도수가 높은 단어들만 사용하기 위해서 단어에 대한 빈도수를 기준으로 정렬한 뒤에 부여합니다.\n",
        "\n",
        "## 1. 정수 인코딩(inter Encoding)\n",
        "\n",
        "- 단어에 정수를 부여하는 방법 중 하나로 단어를 빈도수 순으로 정렬한 단어 집합(vocabulary)을 만들고, 빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여하는 방법이 있습니다.\n",
        "\n",
        "1) Dictionary 사용\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2fFOmT3pxAi"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVy91QNSpxFO"
      },
      "source": [
        "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnLfR-Yfqu58",
        "outputId": "ce2fb2c8-49a5-4230-d353-6fc4a3453442"
      },
      "source": [
        "# 문장 토큰화\n",
        "text = sent_tokenize(text)\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TDbUyxLqw8E"
      },
      "source": [
        "기존의 텍스트 데이터가 문장 단위로 토큰화 된 것을 확인할 수 있습니다. 이제 정제 작업을 병행하며, 단어 토큰화를 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBKFxbxoqyZL",
        "outputId": "72584211-e3fe-4a9f-92e5-c39547bc4d29"
      },
      "source": [
        "# 정제와 단어 토큰화\n",
        "vocab = {} # 파이썬의 dictionary 자료형\n",
        "sentences = []\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "for i in text:\n",
        "    sentence = word_tokenize(i) # 단어 토큰화를 수행합니다.\n",
        "    result = []\n",
        "\n",
        "    for word in sentence: \n",
        "        word = word.lower() # 모든 단어를 소문자화하여 단어의 개수를 줄입니다.\n",
        "        if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어를 제거합니다.\n",
        "            if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거합니다.\n",
        "                result.append(word)\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = 0 \n",
        "                vocab[word] += 1\n",
        "    sentences.append(result) \n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKsCntWdq3UO"
      },
      "source": [
        "텍스트를 숫자로 바꾸는 단계라는 것은 본격적으로 자연어 처리 작업에 들어간다는 의미이므로, 단어가 텍스트일 때만 할 수 있는 최대한의 전처리를 끝내놓아야 합니다. 위의 코드를 보면, 동일한 단어가 대문자로 표기되었다는 이유로 서로 다른 단어로 카운트되는 일이 없도록 모든 단어를 소문자로 바꾸었습니다. 그리고 자연어 처리에서 크게 의미를 갖지 못하는 불용어와 길이가 짧은 단어를 제거하는 방법을 사용하였습니다.\n",
        "\n",
        "현재 vocab에는 중복을 제거한 단어와 각 단어에 대한 빈도수가 기록되어져 있습니다. vocab을 출력해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhBRkb5Fq7gs",
        "outputId": "57d8e7bb-da77-40d8-e308-1e293d2bd4c5"
      },
      "source": [
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc3J0rP8q_A_",
        "outputId": "a16a1386-0323-4823-8cc9-543506ba364a"
      },
      "source": [
        "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "La7hHRpyrA8j",
        "outputId": "84ac92c2-a0e8-4764-c241-2d5f6de1104e"
      },
      "source": [
        "#빈도수가 높은 순서대로 정렬\n",
        "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
        "print(vocab_sorted)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01nKB_9yrEyj",
        "outputId": "8a0cdd44-74a2-432e-bc64-aa642bac9bc0"
      },
      "source": [
        "#높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여\n",
        "word_to_index = {}\n",
        "i=0\n",
        "for (word, frequency) in vocab_sorted :\n",
        "    if frequency > 1 : # 정제(Cleaning) 챕터에서 언급했듯이 빈도수가 적은 단어는 제외한다.\n",
        "        i=i+1\n",
        "        word_to_index[word] = i\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LQaYjGzrLa4"
      },
      "source": [
        "1의 인덱스를 가진 단어가 가장 빈도수가 높은 단어가 됩니다. 그리고 이러한 작업을 수행하는 동시에 각 단어의 빈도수를 알 경우에만 할 수 있는 전처리인 빈도수가 적은 단어를 제외시키는 작업을 합니다. 등장 빈도가 낮은 단어는 자연어 처리에서 의미를 가지지 않을 가능성이 높기 때문입니다. 여기서는 빈도수가 1인 단어들은 전부 제외시켰습니다.\n",
        "\n",
        "자연어 처리를 하다보면, 텍스트 데이터에 있는 단어를 모두 사용하기 보다는 빈도수가 가장 높은 n개의 단어만 사용하고 싶은 경우가 많습니다. 위 단어들은 빈도수가 높은 순으로 낮은 정수가 부여되어져 있으므로 빈도수 상위 n개의 단어만 사용하고 싶다고하면 vocab에서 정수값이 1부터 n까지인 단어들만 사용하면 됩니다. 여기서는 상위 5개 단어만 사용한다고 가정하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIbqlRz6rOfy",
        "outputId": "4b4eea99-fab3-4cfe-bda2-59479fa88e4f"
      },
      "source": [
        "vocab_size = 5\n",
        "words_frequency = [w for w,c in word_to_index.items() if c >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거\n",
        "for w in words_frequency:\n",
        "    del word_to_index[w] # 해당 단어에 대한 인덱스 정보를 삭제\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8dsnzehrTw8"
      },
      "source": [
        "이제 word_to_index에는 빈도수가 높은 상위 5개의 단어만 저장되었습니다. 이제 word_to_index를 사용하여 단어 토큰화가 된 상태로 저장된 sentences에 있는 각 단어를 정수로 바꾸는 작업을 하겠습니다.\n",
        "\n",
        "예를 들어 sentences에서 첫번째 문장은 ['barber', 'person']이었는데, 이 문장에 대해서는 [1, 5]로 인코딩합니다. 그런데 두번째 문장인 ['barber', 'good', 'person']에는 더 이상 word_to_index에는 존재하지 않는 단어인 'good'이라는 단어가 있습니다.\n",
        "\n",
        "이처럼 단어 집합에 존재하지 않는 단어들을 Out-Of-Vocabulary(단어 집합에 없는 단어)의 약자로 'OOV'라고 합니다. word_to_index에 'OOV'란 단어를 새롭게 추가하고, 단어 집합에 없는 단어들은 'OOV'의 인덱스로 인코딩하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfVZ1ffarOiv"
      },
      "source": [
        "word_to_index['OOV'] = len(word_to_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukWR2_64rOlw",
        "outputId": "0516bfdf-fa6f-4534-99bd-1ff752c0bf40"
      },
      "source": [
        "#word_to_index를 사용하여 sentences의 모든 단어들을 맵핑되는 정수로 인코딩\n",
        "\n",
        "encoded = []\n",
        "for s in sentences:\n",
        "    temp = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            temp.append(word_to_index[w])\n",
        "        except KeyError:\n",
        "            temp.append(word_to_index['OOV'])\n",
        "    encoded.append(temp)\n",
        "print(encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6YBwOgYrb_J"
      },
      "source": [
        "### Counter 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXktbN3brOpc"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLPaZ9KsrOsq",
        "outputId": "c3faea4d-2d77-4b05-e76f-cf77b6ebd7bc"
      },
      "source": [
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PCdkPaPrhWa",
        "outputId": "022c8362-cbb7-45b2-a9f5-88fed75f6b5e"
      },
      "source": [
        "# 단어 집합(Vocabulary)을 만들기 위해서 \n",
        "#sentences에서 문장의 경계인 [,]를 제거하고 단어들을 하나의 리스트로 만듬\n",
        "words = sum(sentences, [])\n",
        "# 위 작업은 words = np.hstack(sentences)로도 수행 가능.\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDCUHel4ru3g",
        "outputId": "2e543680-c727-42c6-d0b8-82004052ca9e"
      },
      "source": [
        "vocab = Counter(words) # 파이썬의 Counter 모듈을 이용하면 단어의 모든 빈도를 쉽게 계산할 수 있습니다.\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQWiqzFZry25",
        "outputId": "2d6c7170-620c-4cf8-f244-f5d0e34e2971"
      },
      "source": [
        "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LEPqmpYr00-",
        "outputId": "8665a8f0-7e1f-44b2-fdaf-06f26068dcbc"
      },
      "source": [
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0VQZOlwr1tY",
        "outputId": "fbf9608e-43f3-4179-a593-e762a58d8d0e"
      },
      "source": [
        "# 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여합니다.\n",
        "word_to_index = {}\n",
        "i = 0\n",
        "for (word, frequency) in vocab :\n",
        "    i = i+1\n",
        "    word_to_index[word] = i\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcEQ5HY6r5eK"
      },
      "source": [
        "## NLTK의 FreqDist 사용하기\n",
        "\n",
        "NLTK에서는 빈도수 계산 도구인 FreqDist()를 지원합니다. 위에서 사용한 Counter()랑 같은 방법으로 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN-4UfJTr1wb"
      },
      "source": [
        "from nltk import FreqDist\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwrNH7_Nr10R"
      },
      "source": [
        "# np.hstack으로 문장 구분을 제거하여 입력으로 사용 . ex) ['barber', 'person', 'barber', 'good' ... 중략 ...\n",
        "vocab = FreqDist(np.hstack(sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLy-y9qhr13f",
        "outputId": "9a751252-6ecf-4b82-bfe8-b3585554edac"
      },
      "source": [
        "print(vocab[\"barber\"]) # 'barber'라는 단어의 빈도수 출력"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OX9slH5Lr16r",
        "outputId": "67998164-14b3-45fe-e17e-f3f045473082"
      },
      "source": [
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIx5Pi6at5Ln"
      },
      "source": [
        "앞서 Counter()를 사용했을 때와 결과가 같습니다. 이전 실습들과 마찬가지로 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여합니다. 그런데 이번에는 enumerate()를 사용하여 좀 더 짧은 코드로 인덱스를 부여하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd8ue-5Tr19z",
        "outputId": "1cba0e95-171b-429d-c1be-b2354e00b6fb"
      },
      "source": [
        "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-fTfc7BvCOz"
      },
      "source": [
        "# 한국어 전처리 패키지 (Text preprocessing Tools for Korean Text)z\n",
        "\n",
        "1.PyKoSpacing\n",
        "\n",
        "전희원님이 개발한 PyKoSpacing은 한국어 띄어쓰기 패키지로 띄어쓰기가 되어있지 않은 문장을 띄어쓰기를 한 문장으로 변환해주는 패키지입니다. PyKoSpacing은 대용량 코퍼스를 학습하여 만들어진 띄어쓰기 딥 러닝 모델로 준수한 성능을 가지고 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zA0_dBgruBnn",
        "outputId": "4495515f-06d7-4dbc-a380-e768a7e28af4"
      },
      "source": [
        "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/haven-jeon/PyKoSpacing.git\n",
            "  Cloning https://github.com/haven-jeon/PyKoSpacing.git to /tmp/pip-req-build-em7dtd_i\n",
            "  Running command git clone -q https://github.com/haven-jeon/PyKoSpacing.git /tmp/pip-req-build-em7dtd_i\n",
            "Collecting tensorflow==2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/0a/012cc33c643d844433d13001dd1db179e7020b05ddbbd0a9dc86c38a8efa/tensorflow-2.4.0-cp37-cp37m-manylinux2010_x86_64.whl (394.7MB)\n",
            "\u001b[K     |████████████████████████████████| 394.7MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from pykospacing==0.4) (2.4.3)\n",
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (from pykospacing==0.4) (2.10.0)\n",
            "Collecting argparse>=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.1.2)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.36.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (3.7.4.3)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.32.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (2.4.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.12.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.15.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (3.12.4)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (3.3.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (2.4.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (0.3.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.12)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.19.5)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->pykospacing==0.4) (1.12.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.4.3->pykospacing==0.4) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.4.3->pykospacing==0.4) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow==2.4.0->pykospacing==0.4) (56.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (0.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.28.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.8.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.10.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (1.24.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (0.2.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0->pykospacing==0.4) (0.4.8)\n",
            "Building wheels for collected packages: pykospacing\n",
            "  Building wheel for pykospacing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pykospacing: filename=pykospacing-0.4-cp37-none-any.whl size=2255638 sha256=ea98e3a3dfbcf1618758a19aa9d9ea29a72a8851c697c61f7ac739fcfe361b92\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zq7l3lsm/wheels/4d/45/58/e26cb2b7f6a063d234158c6fd1e5700f6e15b99d67154340ba\n",
            "Successfully built pykospacing\n",
            "Installing collected packages: tensorflow, argparse, pykospacing\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed argparse-1.4.0 pykospacing-0.4 tensorflow-2.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XuxT4OsuBrO"
      },
      "source": [
        "sent = '김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYiLlotnuBug",
        "outputId": "46f0905a-31a4-4a4c-ccd9-0249ae08b3f8"
      },
      "source": [
        "new_sent = sent.replace(\" \", '') # 띄어쓰기가 없는 문장 임의로 만들기\n",
        "print(new_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "김철수는극중두인격의사나이이광수역을맡았다.철수는한국유일의태권도전승자를가리는결전의날을앞두고10년간함께훈련한사형인유연재(김광수분)를찾으러속세로내려온인물이다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9Ew0FGtuB0f",
        "outputId": "2dd9163e-a79a-4b06-89bb-896020301cff"
      },
      "source": [
        "#원 문장과 비교\n",
        "\n",
        "from pykospacing import spacing\n",
        "\n",
        "kospacing_sent = spacing(new_sent)\n",
        "print(sent)\n",
        "print(kospacing_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\n",
            "김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkxcbOzLvW2E"
      },
      "source": [
        "2. Py-Hanspell\n",
        "- Py-Hanspell은 네이버 한글 맞춤법 검사기를 바탕으로 만들어진 패키지입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUTVc4T4uB4g",
        "outputId": "1e45746f-37f6-4471-bbc0-ecebd7a8ae52"
      },
      "source": [
        "!pip install git+https://github.com/ssut/py-hanspell.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ssut/py-hanspell.git\n",
            "  Cloning https://github.com/ssut/py-hanspell.git to /tmp/pip-req-build-w6w83nxa\n",
            "  Running command git clone -q https://github.com/ssut/py-hanspell.git /tmp/pip-req-build-w6w83nxa\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from py-hanspell==1.1) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->py-hanspell==1.1) (2020.12.5)\n",
            "Building wheels for collected packages: py-hanspell\n",
            "  Building wheel for py-hanspell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-hanspell: filename=py_hanspell-1.1-cp37-none-any.whl size=4854 sha256=8ceb0765a5cf7c0e217d7d1a9e267eef67a6ee3b2cbce73b448234fc1e4e019f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ralf50xv/wheels/0a/25/d1/e5e96476dbb1c318cc26c992dd493394fe42b0c204b3e65588\n",
            "Successfully built py-hanspell\n",
            "Installing collected packages: py-hanspell\n",
            "Successfully installed py-hanspell-1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doE5IVV4veCB",
        "outputId": "1021db0e-16e0-48b0-c88a-2ab0ed5216f6"
      },
      "source": [
        "from hanspell import spell_checker\n",
        "\n",
        "sent = \"맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지 \"\n",
        "spelled_sent = spell_checker.check(sent)\n",
        "\n",
        "hanspell_sent = spelled_sent.checked\n",
        "print(hanspell_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "맞춤법 틀리면 왜 안돼? 쓰고 싶은 대로 쓰면 되지\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJl7RuexvhJR",
        "outputId": "5f9f8b6b-24dc-43a9-d8a9-19128ea32076"
      },
      "source": [
        "spelled_sent = spell_checker.check(new_sent)\n",
        "\n",
        "hanspell_sent = spelled_sent.checked\n",
        "print(hanspell_sent)\n",
        "print(kospacing_sent) # 앞서 사용한 kospacing 패키지에서 얻은 결과"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "김철수는 극 중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연제(김광수 분)를 찾으러 속세로 내려온 인물이다.\n",
            "김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-5-dJiuvfoa"
      },
      "source": [
        "이 패키지는 띄어쓰기 또한 보정합니다. PyKoSpacing에 사용한 예제를 그대로 사용해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2HMqvFqvv-R",
        "outputId": "e4cb712a-380b-4f24-b8e8-ae8eb56571dc"
      },
      "source": [
        "spelled_sent = spell_checker.check(new_sent)\n",
        "\n",
        "hanspell_sent = spelled_sent.checked\n",
        "print(hanspell_sent)\n",
        "print(kospacing_sent) # 앞서 사용한 kospacing 패키지에서 얻은 결과"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "김철수는 극 중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연제(김광수 분)를 찾으러 속세로 내려온 인물이다.\n",
            "김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98XDII5Jzw8V"
      },
      "source": [
        "# SOYNLP를 이용한 단어 토큰화\n",
        "\n",
        "soynlp는 품사 태깅, 단어 토큰화 등을 지원하는 단어 토크나이저입니다. 비지도 학습으로 단어 토큰화를 한다는 특징을 갖고 있으며, 데이터에 자주 등장하는 단어들을 단어로 분석합니다. soynlp 단어 토크나이저는 내부적으로 단어 점수 표로 동작합니다. 이 점수는 응집 확률(cohesion probability)과 브랜칭 엔트로피(branching entropy)를 활용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hs-oCl6Sz19M",
        "outputId": "be381f69-4d95-40f8-94d2-497b39cfab7c"
      },
      "source": [
        "!pip install soynlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting soynlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/50/6913dc52a86a6b189419e59f9eef1b8d599cffb6f44f7bb91854165fc603/soynlp-0.0.493-py3-none-any.whl (416kB)\n",
            "\r\u001b[K     |▉                               | 10kB 16.8MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20kB 14.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 30kB 9.4MB/s eta 0:00:01\r\u001b[K     |███▏                            | 40kB 7.7MB/s eta 0:00:01\r\u001b[K     |████                            | 51kB 7.3MB/s eta 0:00:01\r\u001b[K     |████▊                           | 61kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 71kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 81kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 92kB 7.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 102kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 112kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 122kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 133kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 143kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 153kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 163kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 174kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 184kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 194kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 204kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 215kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 225kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 235kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 245kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 256kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 266kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 276kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 286kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 296kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 307kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 317kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 327kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 337kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 348kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 358kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 368kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 378kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 389kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 399kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 409kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 419kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.19.5)\n",
            "Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (5.4.8)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.0.1)\n",
            "Installing collected packages: soynlp\n",
            "Successfully installed soynlp-0.0.493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArRICPrm0GPD"
      },
      "source": [
        "1. 신조어 문제\n",
        "\n",
        "soynlp를 소개하기 전에 기존의 형태소 분석기가 가진 문제는 무엇이었는지, SOYNLP가 어떤 점에서 유용한지 정리해봅시다. 기존의 형태소 분석기는 신조어나 형태소 분석기에 등록되지 않은 단어 같은 경우에는 제대로 구분하지 못하는 단점이 있었습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfxN9nfl0Jiy",
        "outputId": "ea660016-0c40-4f19-a625-26e21a132bc8"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "tokenizer = Okt()\n",
        "print(tokenizer.morphs('에이비식스 이대휘 1월 최애돌 기부 요정'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['에이', '비식스', '이대', '휘', '1월', '최애', '돌', '기부', '요정']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZiKWdSO0Lgc"
      },
      "source": [
        "에이비식스는 아이돌의 이름이고, 이대휘는 에이비식스의 멤버이며, 최애돌은 최고로 애정하는 캐릭터라는 뜻이지만 위의 형태소 분석 결과에서는 전부 분리된 결과를 보여줍니다.\n",
        "\n",
        "그렇다면 텍스트 데이터에서 특정 문자 시퀀스가 함께 자주 등장하는 빈도가 높고, 앞 뒤로 조사 또는 완전히 다른 단어가 등장하는 것을 고려해서 해당 문자 시퀀스를 형태소라고 판단하는 단어 토크나이저라면 어떨까요?\n",
        "\n",
        "예를 들어 에이비식스라는 문자열이 자주 연결되어 등장한다면 한 단어라고 판단하고, 또한 에이비식스라는 단어 앞, 뒤에 '최고', '가수', '실력'과 같은 독립된 다른 단어들이 계속해서 등장한다면 에이비식스를 한 단어로 파악하는 식이지요. 그리고 이런 아이디어를 가진 단어 토크나이저가 soynlp입니다.\n",
        "\n",
        "\n",
        "2. 학습하기\n",
        "\n",
        "soynlp는 기본적으로 학습에 기반한 토크나이저이므로 학습에 필요한 한국어 문서를 다운로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozzTo7h00K_T"
      },
      "source": [
        "import urllib.request\n",
        "from soynlp import DoublespaceLineCorpus\n",
        "from soynlp.word import WordExtractor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1DZw3At0XG5",
        "outputId": "0f1f0d55-f2de-4a9f-e2bc-b391136ea135"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt\", filename=\"2016-10-20.txt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2016-10-20.txt', <http.client.HTTPMessage at 0x7f88b1b36d10>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX_FhooW0SLV",
        "outputId": "6276c10f-ad2e-4010-95b1-b1b87fd69b1c"
      },
      "source": [
        "# 훈련 데이터를 다수의 문서로 분리\n",
        "corpus = DoublespaceLineCorpus(\"2016-10-20.txt\")\n",
        "len(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30091"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKv814Vf0bs-",
        "outputId": "0bc8857e-a7cc-4345-8fab-905ee760e374"
      },
      "source": [
        "i = 0\n",
        "for document in corpus:\n",
        "  if len(document) > 0:\n",
        "    print(document)\n",
        "    i = i+1\n",
        "  if i == 3:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19  1990  52 1 22\n",
            "오패산터널 총격전 용의자 검거 서울 연합뉴스 경찰 관계자들이 19일 오후 서울 강북구 오패산 터널 인근에서 사제 총기를 발사해 경찰을 살해한 용의자 성모씨를 검거하고 있다 성씨는 검거 당시 서바이벌 게임에서 쓰는 방탄조끼에 헬멧까지 착용한 상태였다 독자제공 영상 캡처 연합뉴스  서울 연합뉴스 김은경 기자 사제 총기로 경찰을 살해한 범인 성모 46 씨는 주도면밀했다  경찰에 따르면 성씨는 19일 오후 강북경찰서 인근 부동산 업소 밖에서 부동산업자 이모 67 씨가 나오기를 기다렸다 이씨와는 평소에도 말다툼을 자주 한 것으로 알려졌다  이씨가 나와 걷기 시작하자 성씨는 따라가면서 미리 준비해온 사제 총기를 이씨에게 발사했다 총알이 빗나가면서 이씨는 도망갔다 그 빗나간 총알은 지나가던 행인 71 씨의 배를 스쳤다  성씨는 강북서 인근 치킨집까지 이씨 뒤를 쫓으며 실랑이하다 쓰러뜨린 후 총기와 함께 가져온 망치로 이씨 머리를 때렸다  이 과정에서 오후 6시 20분께 강북구 번동 길 위에서 사람들이 싸우고 있다 총소리가 났다 는 등의 신고가 여러건 들어왔다  5분 후에 성씨의 전자발찌가 훼손됐다는 신고가 보호관찰소 시스템을 통해 들어왔다 성범죄자로 전자발찌를 차고 있던 성씨는 부엌칼로 직접 자신의 발찌를 끊었다  용의자 소지 사제총기 2정 서울 연합뉴스 임헌정 기자 서울 시내에서 폭행 용의자가 현장 조사를 벌이던 경찰관에게 사제총기를 발사해 경찰관이 숨졌다 19일 오후 6시28분 강북구 번동에서 둔기로 맞았다 는 폭행 피해 신고가 접수돼 현장에서 조사하던 강북경찰서 번동파출소 소속 김모 54 경위가 폭행 용의자 성모 45 씨가 쏜 사제총기에 맞고 쓰러진 뒤 병원에 옮겨졌으나 숨졌다 사진은 용의자가 소지한 사제총기  신고를 받고 번동파출소에서 김창호 54 경위 등 경찰들이 오후 6시 29분께 현장으로 출동했다 성씨는 그사이 부동산 앞에 놓아뒀던 가방을 챙겨 오패산 쪽으로 도망간 후였다  김 경위는 오패산 터널 입구 오른쪽의 급경사에서 성씨에게 접근하다가 오후 6시 33분께 풀숲에 숨은 성씨가 허공에 난사한 10여발의 총알 중 일부를 왼쪽 어깨 뒷부분에 맞고 쓰러졌다  김 경위는 구급차가 도착했을 때 이미 의식이 없었고 심폐소생술을 하며 병원으로 옮겨졌으나 총알이 폐를 훼손해 오후 7시 40분께 사망했다  김 경위는 외근용 조끼를 입고 있었으나 총알을 막기에는 역부족이었다  머리에 부상을 입은 이씨도 함께 병원으로 이송됐으나 생명에는 지장이 없는 것으로 알려졌다  성씨는 오패산 터널 밑쪽 숲에서 오후 6시 45분께 잡혔다  총격현장 수색하는 경찰들 서울 연합뉴스 이효석 기자 19일 오후 서울 강북구 오패산 터널 인근에서 경찰들이 폭행 용의자가 사제총기를 발사해 경찰관이 사망한 사건을 조사 하고 있다  총 때문에 쫓던 경관들과 민간인들이 몸을 숨겼는데 인근 신발가게 직원 이모씨가 다가가 성씨를 덮쳤고 이어 현장에 있던 다른 상인들과 경찰이 가세해 체포했다  성씨는 경찰에 붙잡힌 직후 나 자살하려고 한 거다 맞아 죽어도 괜찮다 고 말한 것으로 전해졌다  성씨 자신도 경찰이 발사한 공포탄 1발 실탄 3발 중 실탄 1발을 배에 맞았으나 방탄조끼를 입은 상태여서 부상하지는 않았다  경찰은 인근을 수색해 성씨가 만든 사제총 16정과 칼 7개를 압수했다 실제 폭발할지는 알 수 없는 요구르트병에 무언가를 채워두고 심지를 꽂은 사제 폭탄도 발견됐다  일부는 숲에서 발견됐고 일부는 성씨가 소지한 가방 안에 있었다\n",
            "테헤란 연합뉴스 강훈상 특파원 이용 승객수 기준 세계 최대 공항인 아랍에미리트 두바이국제공항은 19일 현지시간 이 공항을 이륙하는 모든 항공기의 탑승객은 삼성전자의 갤럭시노트7을 휴대하면 안 된다고 밝혔다  두바이국제공항은 여러 항공 관련 기구의 권고에 따라 안전성에 우려가 있는 스마트폰 갤럭시노트7을 휴대하고 비행기를 타면 안 된다 며 탑승 전 검색 중 발견되면 압수할 계획 이라고 발표했다  공항 측은 갤럭시노트7의 배터리가 폭발 우려가 제기된 만큼 이 제품을 갖고 공항 안으로 들어오지 말라고 이용객에 당부했다  이런 조치는 두바이국제공항 뿐 아니라 신공항인 두바이월드센터에도 적용된다  배터리 폭발문제로 회수된 갤럭시노트7 연합뉴스자료사진\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLNlJfkO0gI9",
        "outputId": "5cd04fde-d61f-41ec-9b56-bb4f11784eec"
      },
      "source": [
        "word_extractor = WordExtractor()\n",
        "word_extractor.train(corpus)\n",
        "word_score_table = word_extractor.extract()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training was done. used memory 2.485 Gb\n",
            "all cohesion probabilities was computed. # words = 223348\n",
            "all branching entropies was computed # words = 361598\n",
            "all accessor variety was computed # words = 361598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tenps97U0eSi"
      },
      "source": [
        "정상 출력되는 것을 확인하였습니다. soynlp는 학습 기반의 단어 토크나이저이므로 기존의 KoNLPy에서 제공하는 형태소 분석기들과는 달리 학습 과정을 거쳐야 합니다. 이는 전체 코퍼스로부터 응집 확률과 브랜칭 엔트로피 단어 점수표를 만드는 과정입니다. WordExtractor.extract()를 통해서 전체 코퍼스에 대해 단어 점수표를 계산합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eOcZRpq5Vdy",
        "outputId": "4a2949e2-c570-4321-9be1-024b76fbf899"
      },
      "source": [
        "word_extractor = WordExtractor()\n",
        "word_extractor.train(corpus)\n",
        "word_score_table = word_extractor.extract()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training was done. used memory 2.566 Gb\n",
            "all cohesion probabilities was computed. # words = 223348\n",
            "all branching entropies was computed # words = 361598\n",
            "all accessor variety was computed # words = 361598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXAaTvPR5Zvn"
      },
      "source": [
        "# SOYNLP의 응집 확률(cohesion probability)\n",
        "\n",
        "응집 확률은 내부 문자열(substring)이 얼마나 응집하여 자주 등장하는지를 판단하는 척도입니다. 응집 확률은 문자열을 문자 단위로 분리하여 내부 문자열을 만드는 과정에서 왼쪽부터 순서대로 문자를 추가하면서 각 문자열이 주어졌을 때 그 다음 문자가 나올 확률을 계산하여 누적곱을 한 값입니다. 이 값이 높을수록 전체 코퍼스에서 이 문자열 시퀀스는 하나의 단어로 등장할 가능성이 높습니다. 수식은 아래와 같습니다.\n",
        "\n",
        "<figure>\n",
        "<img src = 'https://wikidocs.net/images/page/92961/%EC%88%98%EC%8B%9D.png'>\n",
        "<figure>\n",
        "\n",
        "'반포한강공원에'라는 7의 길이를 가진 문자 시퀀스에 대해서 각 내부 문자열의 스코어를 구하는 과정은 아래와 같습니다.\n",
        "\n",
        "<figure>\n",
        "<img src = 'https://wikidocs.net/images/page/92961/%EC%88%98%EC%8B%9D2.png'>\n",
        "<figure>\n",
        "\n",
        "실습을 통해 직접 응집 확률을 계산해보겠습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI3ldGli5yOL",
        "outputId": "b4a4e565-0650-444f-b8dd-95eeb13e3f1e"
      },
      "source": [
        "word_score_table[\"반포한\"].cohesion_forward"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08838002913645132"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FJyxKWx51NQ"
      },
      "source": [
        "그렇다면 '반포한강'의 응집 확률은 '반포한'의 응집 확률보다 높을까요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tb-LQ5C54Wu",
        "outputId": "13570bdc-9679-40d1-a40f-aed4aacb68e9"
      },
      "source": [
        "word_score_table[\"반포한강\"].cohesion_forward"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.19841268168224552"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYI4plFD56IZ"
      },
      "source": [
        "'반포한강'은 '반포한'보다 응집 확률이 높습니다. 그렇다면 '반포한강공'은 어떨까요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d564rhpI58Tx",
        "outputId": "bc69273b-5034-4a45-c478-63468989c6ee"
      },
      "source": [
        "word_score_table[\"반포한강공\"].cohesion_forward"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2972877884078849"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh5_y1nh5-ar"
      },
      "source": [
        "역시나 '반포한강'보다 응집 확률이 높습니다. '반포한강공원'은 어떨까요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJeNLZlN6AOP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf653b2a-6cb9-4091-8ead-79a6b134fe4a"
      },
      "source": [
        "word_score_table[\"반포한강공원\"].cohesion_forward"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.37891487632839754"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gySYEHLX6BxT"
      },
      "source": [
        "'반포한강공'보다 응집 확률이 높습니다. 여기다가 조사 '에'를 붙인 '반포한강공원에'는 어떨까요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhQGhujV6Dp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a53091-4843-4638-9063-472d620aa6e7"
      },
      "source": [
        "word_score_table[\"반포한강공원에\"].cohesion_forward"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.33492963377557666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQMfrAgW6FTB"
      },
      "source": [
        "오히려 '반포한강공원'보다 응집도가 낮아집니다. 결국 결합도는 '반포한강공원'일 때가 가장 높았습니다. 응집도를 통해 판단하기에 하나의 단어로 판단하기에 가장 적합한 문자열은 '반포한강공원'이라고 볼 수 있겠네요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXjjzD0W6a3Z"
      },
      "source": [
        "참고 :https://datascienceschool.net/03%20machine%20learning/03.01.05%20%ED%99%95%EB%A5%A0%EB%A1%A0%EC%A0%81%20%EC%96%B8%EC%96%B4%20%EB%AA%A8%ED%98%95.html\n",
        "# 확률론적 언어 모형\n",
        "확률론적 언어 모형(Probabilitstic Language Model)은 m개의 단어 $w_1,w_2,\\dots,w_m$ 열(word sequenace)이 주어졌을 때 문장으로써 성립될 확률 $P(w_1,w_2,\\dots,w_m)$을 출력함으로써 이 단어 열이 실제로 현실에서 사용될 수 있는 문장(sentence)인지를 판별하는 모형\n",
        "\n",
        "$P(w_1,w_2,\\dots,w_m) = P(w_1,w_2, \\dots, w_{m-1})~{\\cdot}~ P(w_m {\\mid}w_1,w_2, \\dots,w_{m-1}) $\n",
        "\n",
        "$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~=P(w_1,w_2,\\dots,w_{m-2})~{\\cdot}~ P(w_{m-1}|w_1,w_2,\\dots,w_{m-2}) ~{\\cdot}~ P(w_{m}|w_1,w_2,\\dots,w_{m-1}) $\n",
        "\n",
        "$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~=P(w_1)~{\\cdot}~P(w_2 {\\mid} w_1)~{\\cdot}~ P(w_3 {\\mid} w_1,w_2)~{\\cdot}~P(w_4 {\\mid} w_1,w_2,w_3)~{\\cdots}~ P(w_4 {\\mid} w_1,w_2,w_3)$\n",
        "\n",
        "\n",
        "여기서 $P(w_m {\\mid}w_1,w_2, \\dots,w_{m-1})$ 은 지금까지 $w_1,w_2,\\cdots,w_{m-1}$라는 단어 열이 나왔을 때, 그 다음 단어로 $w_m$이 나올 조건부 확률을 말함. 여기에서 지금까지 나온 단어를 문맥(context) 정보라고 한다.\n",
        "\n",
        "\n",
        "## 유니그램 모형\n",
        "- 만약 모든 단어의 활용이 완전히 서로 독립이라면, 단어 열의 확률은 다음과 같이 각 단어의 확률의 곱이 된다. 이러한 모형을 유니그램 모형이라고 한다.\n",
        "\n",
        "$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~P(w_1,w_2,\\cdots,w_m) = \\Pi_{i=1}^m P(w_i)$\n",
        "\n",
        "## 바이그램 모형\n",
        "- 만약 단어의 활용이 바로 전 단어에만 의존한다면 단어 열의 확률은 다음과 같다. 이러한 모형을 바이그램 모형 또는 마코프 모형(markov model)이라고 한다.\n",
        "\n",
        "$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~P(w_1,w_2,\\cdots,w_m) = P(w_1) \\Pi_{i=2}^m P(w_i {\\mid} w_{i-1})$\n",
        "\n",
        "## N-gram 모형\n",
        "\n",
        "- 만약 단어의 활용이 바로 전 $n-1$개의 단어에만 의존한다면 단어 열의 확률은 다음과 같다. 이러한 모형을 N그램 모형이라고 한다.\n",
        "\n",
        "$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~P(w_1,w_2,\\cdots,w_m) = P(w_1) \\Pi_{i=n}^m P(w_i {\\mid} w_{i-1}, \\cdots,w_{i-n})$"
      ]
    }
  ]
}